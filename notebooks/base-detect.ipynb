{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event Detection\n",
    "\n",
    "This notebooks shows an example of event detection on an MD LJ system.\n",
    "The notebook will go through the data processing and detection pipeline.\n",
    "\n",
    "### Import All the Things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bottleneck as bn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ruptures as rpt  # change point detection library\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import sklearn as sk\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import dupin as ed\n",
    "\n",
    "FILENAME = \"lj-data.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Trajectory Data\n",
    "\n",
    "We also normalize all features to a range of $[0, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf(FILENAME, \"data\")\n",
    "df = pd.DataFrame(\n",
    "    MinMaxScaler().fit_transform(df.to_numpy()),\n",
    "    columns=df.columns,\n",
    "    index=df.index)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "In order to lower the dimension of the data for interpretability and\n",
    "improved detection, we will apply some feature selection methods to\n",
    "the data.\n",
    "\n",
    "### Mean Shift\n",
    "\n",
    "First, we remove features where there is no detectable shift in the\n",
    "mean between the start and end of the trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_shift_filter = ed.preprocessing.filter.MeanShift(sensitivity=1e-4, )\n",
    "data = df.to_numpy()\n",
    "print(f\"Starting number of features: {data.shape[1]}\")\n",
    "data = mean_shift_filter(data, sample_size=6)\n",
    "print(f\"New number of features: {data.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlated\n",
    "\n",
    "We now take all features with a mean shift and cluster then\n",
    "based on their correlation with other features. We will then\n",
    "select 2 features for each cluster. The 2 features are chosen\n",
    "based on a provided or random feature importance. In the next\n",
    "cell we use 3 measures of feature importance:\n",
    "1. The noise of the signal through a rolling standard deviation\n",
    "   and mean.\n",
    "2. The likelihoods of the observed mean shift from the `MeanShift`\n",
    "   instance above.\n",
    "3. The degree to which the feature is approximated by a linear\n",
    "   spline with knots sparsely spaced in the signal.\n",
    "\n",
    "The following cell clusters and selects features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average(*arrs, weights=None):\n",
    "    avg_array = np.vstack(arrs)\n",
    "    if weights is None:\n",
    "        weights = np.ones(len(arrs)) / len(arrs)\n",
    "    else:\n",
    "        weights = np.asarray(weights)\n",
    "    return np.sum(weights[:, None] * avg_array, axis=0)\n",
    "    \n",
    "# Rank based on local noise of signal\n",
    "noise_importance = ed.preprocessing.filter.noise_importance(\n",
    "    signal=data, window_size=4)\n",
    "\n",
    "# Rank based on mean shift analysis\n",
    "likelihoods = mean_shift_filter.likelihoods_[\n",
    "    mean_shift_filter.filter_]\n",
    "likelihood_importance = ed.preprocessing.filter.mean_shift_importance(\n",
    "    likelihoods)\n",
    "\n",
    "# Rank based on function smoothness\n",
    "smoothness_importance = ed.preprocessing.filter.local_smoothness_importance(\n",
    "    signal=data, dim=1, spacing=3)\n",
    "\n",
    "importances = [\n",
    "    noise_importance,\n",
    "    likelihood_importance,\n",
    "    smoothness_importance,\n",
    "]\n",
    "    \n",
    "feature_score = weighted_average(*importances, weights=(0.25, 0.25, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "correlation_filter = ed.preprocessing.filter.Correlated(max_clusters=6)\n",
    "\n",
    "correlation_filter(\n",
    "    data,\n",
    "    features_per_cluster=2,\n",
    "    feature_importance=feature_score,\n",
    "    return_filter=True,\n",
    ")\n",
    "\n",
    "final_filter = np.flatnonzero(mean_shift_filter.filter_)[correlation_filter.filter_]\n",
    "\n",
    "pruned_df = pd.DataFrame(\n",
    "    df.iloc[:, final_filter],\n",
    "    index=df.index,\n",
    "    columns=df.columns[final_filter]\n",
    ")\n",
    "print(f\"Number of clusters: {correlation_filter.n_clusters_}\")\n",
    "pruned_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Point Detection\n",
    "\n",
    "Now using the remaining feature we will attempt to detect any events in the\n",
    "trajectory. To start will we check up to 12 change points and find the elbow\n",
    "in the cost for entire signal given the $n$ change points using the KNEEDLE\n",
    "algorithm. For the cost function we will use the built in `CostLinearFit`\n",
    "which fits each dimension versus time to a line. We use the\n",
    "`ruptures.Dynp` dynamic programming global optimizer for detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_regress_cost = ed.detect.offline.CostLinearFit()\n",
    "dynp = rpt.Dynp(custom_cost=lin_regress_cost)\n",
    "sweep_detector = ed.detect.offline.SweepDetector(dynp, max_change_points=12)\n",
    "change_points = sweep_detector.fit(pruned_df.to_numpy())\n",
    "print(f\"Optimal change points: {change_points}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot cost versus number of change points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.plot(sweep_detector.costs_)\n",
    "min_ = min(sweep_detector.costs_)\n",
    "max_ = max(sweep_detector.costs_)\n",
    "ax.vlines(sweep_detector.opt_n_change_points_, min_, max_, \"k\", \"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_with_strs = [\"2}$\", \"4}$\", \"6}$\", \"8}$\", \"10}$\", \"{vor}$\"]\n",
    "fig, axes = plt.subplots(3, 2, figsize=(10, 10))\n",
    "for ax, end_str in zip(axes.ravel(), end_with_strs):\n",
    "    filtered_df = df.filter(\n",
    "        list(filter(lambda x: x.endswith(end_str), df.columns)), axis=\"columns\"\n",
    "    )\n",
    "    max_, min_ = max(filtered_df.max()), min(filtered_df.min())\n",
    "    filtered_df.plot(\n",
    "        ax=ax, legend=False, title=\"_\".join(filtered_df.columns[0].split(\"_\")[-2:])\n",
    "    )\n",
    "    ax.title.set_size(20)\n",
    "    ax.vlines(sweep_detector.opt_change_points_, max_, min_, linestyles=\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot only used features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_with_strs = [\"2}$\", \"4}$\", \"6}$\", \"8}$\", \"10}$\", \"{vor}$\"]\n",
    "n_axes = 0\n",
    "for end_str in end_with_strs:\n",
    "    filtered_df = pruned_df.filter(\n",
    "        list(filter(lambda x: x.endswith(end_str), pruned_df.columns)), axis=\"columns\"\n",
    "    )\n",
    "    if filtered_df.shape[1] != 0:\n",
    "        n_axes += 1\n",
    "\n",
    "rows = int(np.ceil(n_axes / 2))\n",
    "fig, axes = plt.subplots(rows, 2, figsize=(10, 4 * rows))\n",
    "axes = iter(axes.ravel())\n",
    "ax = next(axes)\n",
    "for end_str in end_with_strs:\n",
    "    filtered_df = pruned_df.filter(\n",
    "        list(filter(lambda x: x.endswith(end_str), pruned_df.columns)), axis=\"columns\"\n",
    "    )\n",
    "    if filtered_df.shape[1] != 0:\n",
    "        filtered_df.plot(\n",
    "            ax=ax, legend=False, title=\"_\".join(filtered_df.columns[0].split(\"_\")[-2:])\n",
    "        )\n",
    "        ax.title.set_size(20)\n",
    "        max_, min_ = filtered_df.max().max(), filtered_df.min().min()\n",
    "        ax.vlines(sweep_detector.opt_change_points_, max_, min_, linestyles=\"--\")\n",
    "        try:\n",
    "            ax = next(axes)\n",
    "        except StopIteration:\n",
    "            pass\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_df.to_hdf(\"lj-data.h5\", \"pruned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Research (conda)",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
